---
title: "UT Invert Data Cleaning"
author: "Gordon Gianniny"
date: "2023-05-18"
output: html_document
---

```{r include=FALSE}
#Loading packages:
library(tidyverse) #For data wrangling
library(lubridate) #For working with dates
library(ggplot2) #Plotting
library(RColorBrewer)
library(patchwork)
library(stringr)
library(vegan)
library(broom)
library(sp)
library(geonames) #for getting elevation data
library(rgbif) #for getting elevation data

library(rgdal) #for reading in shapefiles
library(sf) #for intersecting points with shapefile
```


## Overview

This script cleans and organizes aquatic invertebrate data obtained from the National Aquatic Monitoring Center (NAMC) for the state of Utah. The two input data files are: 

  * **Utah_Sample_Data_Final_comids.csv** - sample metadata for all samples. Key columns are: 
  
      - sampleId (unique to each sample), customerAbbreviation (unique to each sampling agency), siteID (unique to each site), siteLongitude and siteLatitude, sampleDate, sampleType, sampleMethod, sampleMethodId, habitatName, habitatID, area, fieldSplit, labSplit, mesh, COMID (unique to each stream segment)

  * **Utah_Taxa_Data_Final.csv** - taxa data for all samples. Key columns are: 
  
    - organismId (unique to each organism), sampleId, taxonomyId (unique to each taxa), levelId (unique to each classification level), levelName (finest level of classification), lifeStageId (unique to each lifestage), lifeStage, lifeStageAbbreviation, splitCount, labSplit, fieldSplit
    
Data cleaning operations carried out in this script are: 

1. Get rid of unneccesary columns in each dataset, then combine the two datasets (merge by sampleID)
  
2. Account for differences in lab split, sampling methods, etc. 
  
    A. Check lab split proportions across the dataset - what are the lab split values/bounds? Set a cutoff.  
  
    B. Sampling method, life stage, habitat type, and mesh size
  
    C. Classification levels - How much of the data is classified to genus? (Keeping all for now)
  
3. Calculate richness & deal with replicates:
  
    A. Calculate richness for each sample
  
    B. Average "true" replicates (within a day/ within 5 days)
  
    C. Average within-year (same siteId) samples
    
    D Average within-reach (same COMID) sites
  
4. Identify and extract sites with long-term (4+ year) data
  
    A. By site
  
    B. By COMID

---
---

## I. Initial data cleaning objectives: 

### 1. Get rid of unneccesary columns in each dataset, then combine the two datasets (merge by sampleID)

**Metadata:** 

Read in the datasets, select key columns listed in intro: *(Note - also adjusting date formatting of sampleDate column to fix dates in the mid-90s coming in as dates in the future - e.g. instead of 1965-01-01, 2065-01-01)*

```{r}
metadata <- read.csv("raw_data/Utah_Sample_Data_Final_comids.csv")%>% #read in datafile
  select("sampleId" , "customerAbbreviation", "siteId", "siteLongitude", "siteLatitude", "sampleDate", "sampleType", "sampleMethod", "sampleMethodId", "habitatName", "habitatId", "area", "fieldSplit", "labSplit", "mesh", "COMID" )%>% #Selecting desired columns
  mutate(sampleDate = as.Date(sampleDate, format ="%m/%d/%y"),  #recode as date
         flag = ifelse(
           sampleDate > today(), 
           "N", "Y"), #flag all rows with dates greater than today
         sampleDate = as.character(sampleDate), #convert back to character to substring
         sampleDate= ifelse(flag == "N", str_replace(sampleDate, "20", "19"), sampleDate), #substring: correct century (replace "20" with "19") for all flagged rows 
         sampleDate = ymd(sampleDate), #convert back to date format. 
         yr = year(sampleDate), #add year column
         sampleProp = labSplit*fieldSplit) #calculate sample proportion
           

taxa <- read.csv("raw_data/Utah_Taxa_Data_Final.csv")%>% #read in datafile
  select("organismId" , "sampleId", "taxonomyId", "scientificName", "levelId", "levelName", "phylum", "class", "order", "family", "subFamily", "genus", "species",  "lifeStageId", "lifeStage", "lifeStageAbbreviation", "splitCount", "labSplit", "fieldSplit") #selecting desired columns
```


Checking # samples in each dataframe: 

```{r}
length(unique(metadata$sampleId)) #number unique sample ID's in metadata
length(unique(taxa$sampleId)) #number unique sample ID's in taxa  data
```

160 more sample ID's in the metadata than in the taxa data, so should have metadata for all taxa samples. 

Merge the dataframes and re-code relevant colums as different classes: result should be 185,366 rows x 32 columns

```{r}
taxa_meta <- merge(taxa, metadata)%>%#merge
  mutate(
    customerAbbreviation = as.factor(customerAbbreviation), 
    levelName = as.factor(levelName), #recoding categorical columns as factors
    lifeStage = as.factor(lifeStage), #recoding categorical columns as factors
    #sampleDate = mdy(sampleDate), #recoding date column in r date format
    sampleType = as.factor(sampleType), #recoding categorical columns as factors
    sampleMethod = as.factor(sampleMethod), #recoding categorical columns as factors
    habitatName = as.factor(habitatName), #recoding categorical columns as factors
    habitatId = as.factor(habitatId)#recoding categorical columns as factors
  )
  
dim(taxa_meta) #check dimensions 

range(taxa_meta$sampleDate) #confirming that date range is believable
```

Save this dataframe: 

```{r}
write.csv(taxa_meta, "raw_data/ut_metadata_taxa_combined.csv")
```

### 2. Accounting for differences/checking accuracy in lab split, sampling methods, area, etc. 

#### A. Sample Proportion (Lab*Field split) - what's the range? Do we need a cutoff? 

First pass, display a basic summary of sample proportion: 

```{r}
summary(metadata$sampleProp)
```
  
Looks like there are a fair number of low sample proportions, including some zeros - plotting: 

```{r}
sampleprop.hist <- ggplot(metadata, aes(x = sampleProp))+ #x axis = labSplit
  geom_histogram(color = "Black", fill = "Grey", bins = 70)+ 
  theme_classic()+
  geom_vline(xintercept = 0.055, linetype = "dashed", color = "Red")+ #add vertical line at 0.055 (cutoff used in Rumschlag et al.)
  labs(x = "Sample Proportion", y = "Count")
sampleprop.hist
```

`r round((nrow(filter(metadata, sampleProp < 0.055))/nrow(metadata))*100, 2)` percent of samples (n = `r nrow(filter(metadata, sampleProp < 0.055))`) are below the 0.055 cutoff from Rumschlag et al (dashed red line). 


##### i. How many individuals are in the samples with sample proportion == 1? 

First, extract samples with sample proportion == 1 and calculate total sample abundance:

```{r}
ls1 <- taxa_meta %>%
  filter(sampleProp == 1)%>% #extract rows w/ sample proportion = 1
  group_by(sampleId)%>% #group by sample ID
  summarise(sample_abund = sum(splitCount)) #calculate total abundance for each sample
```

Basic summary:

```{r}
summary(ls1$sample_abund)
```

Histogram: 

```{r}
ls1.hist <- ggplot(ls1, aes(x = sample_abund))+
  geom_histogram(color = "Black", fill = "Grey", bins = 70)+ 
  theme_classic()+
  annotate(geom = "label", x = 3000, y = 350, label = "Min = 1, Mean = 310, Max = 5,942")+
   annotate(geom = "label", x = 3000, y = 300, label = "32 samples w/ >1000 individuals; 11 samples w/ 1 individual")+
  labs(x = "Total Sample Abundance", y = "Count")
ls1.hist
```

Very strong left skew - most samples have fewer than ~1000 individuals. 

```{r}
length(which(ls1$sample_abund>1000))
length(which(ls1$sample_abund==1))
```

Only 32 samples with > 1000 individuals; 11 samples with only 1 individual. 

Save: 

```{r}
ggsave("plots/ls1_hist.pdf", ls1.hist, device = "pdf", units = "in", height = 5, width = 6.5, dpi = "retina")
```

##### ii. Applying sample proportion cutoff:

What are the tradeoffs between # samples and sample proportion cutoff? 

Plot showing total # samples on Y, lab split on X. 

```{r}
n<- nrow(metadata)

ggplot(metadata, aes(x = sampleProp))+
  stat_ecdf(aes(y = (1-after_stat(y))), geom = "line", size = 1, color = "red")+
  scale_y_continuous("Proportion of samples w/ Sample Proportion > x", position = "left", sec.axis = sec_axis(name = "# Samples w/ Sample Proportion > x", trans = ~.x*n))+
  theme_linedraw()+
  labs(x ="Sample Proportion")+
  geom_vline(xintercept = 0.055, linetype = "dashed")
  
```

For now - using the same cutoff as Rumschlag et al: 

```{r}
taxa_meta_sampleprop <- taxa_meta %>%
  filter(!sampleProp<0.055) #remove all observations with sampleProp < 0.055
```

Using this cutoff means we lose `r round(((nrow(taxa_meta)-nrow(taxa_meta_sampleprop))/nrow(taxa_meta))*100, 2)` percent of observations (`r nrow(taxa_meta)-nrow(taxa_meta_sampleprop)` individual observations). 

#### B. Sampling Method, Habitat type, life stage, and mesh size: 

First, how much variation is there in agency, sampling method, habitat type, etc? 

```{r}
categoricals <- taxa_meta_sampleprop %>% 
  select_if(is.factor)%>% #extract all factor variables
  summarise_all(n_distinct)%>% #count # distinct levels in each factor
  mutate(n = row_number())%>% #extra col for pivot
  pivot_longer(!n, names_to = "variable", values_to = "n_levels")%>% #pivot longer to get column for variable names, column for # levels
  select(!n) #drop extra column
categoricals
```

16 different levels of identification, 6 life stages, 49 "customers" all samples are benthic, 3 sample methods and habitat types. Will probably want to check for variation between:

  * Different sampling methods
  
  * Different habitat types
  
  * Different agencies
  
  * Different mesh sizes (?)
  
Will also probably want to standardize which lifestages we use (ie include adults, or not?) and which level of classification we use (e.g. Rumschlag et al threw out anything not classified at least to genus)

**Rules as of 05-23**:

  * Keep all sampling methods, but keep in dataset throughout calculations. 
  
  * Keep all habitat types
  
  * Ignore agency for now
  
  * Keep all mesh sizes
  
  * Drop egg & exuvia; count adults for non-insects and insects in orders coleoptera & hemiptera. 
  
  * Assume that "unspecified" samples are larvae if insects, adults if non-insects - keep in dataset. 
  
  * Keep all levels of classification. 

**Identify different categories in each of these variables, how much data is in each, and clean up data according to the rules above:**


##### i. Life Stage: Drop exuvia, eggs, and unspecified

```{r}
ggplot(taxa_meta_sampleprop, aes(x = lifeStage, fill = lifeStage))+
  geom_bar(stat = "count", color = "Black")+
  theme_classic()+
  labs(x = "Life Stage", y = "No. of Observations")+
  theme(legend.position = "none")
```

Almost all larvae and, some adults. Very few eggs and exuvia. 

Drop eggs & exuvia; assuming that unspecified samples are adults if non-insects, larvae if insects. Drop all insect adults unless in orders Coleoptera or Hemiptera.

First, make a list of the insect orders for which we want to drop observations of adults:

```{r}
insect_orders <- taxa_meta_sampleprop%>%
  select(class, order)%>% #select class and order columns
  filter(class == "Insecta")%>% #filter all orders in class Insecta
  distinct()%>% #remove duplicates
  filter(!order == "Coleoptera", !order == "Hemiptera") #remove orders we want to keep (Coleoptera, Hemiptera)

to.drop <- insect_orders$order #create vector of order names to drop
```

Next, drop all eggs, exuvia, and adult insects in the to.drop list of orders:

```{r}
taxa_meta_stage <- taxa_meta_sampleprop%>%
  filter(!lifeStage=="Egg", #drop all eggs
         !lifeStage=="Exuvia")%>% #drop all exuvia
  filter(!(lifeStage=="Adult"&order%in%to.drop)) #drop all adults from the orders in the to.drop list
```

##### ii. Sampling Method - Keep all for now:

```{r}
ggplot(taxa_meta_stage, aes(x = sampleMethod, fill = sampleMethod))+
  geom_bar(stat = "count", color = "Black")+
  theme_classic()+
  labs(x = "Sample Method", y = "No. of Observations")+
  theme(legend.position = "none")
```

Fairly evenly distibuted, lots of samples with each method. 

##### iii. Habitat Type - Keep all for now:

```{r}
ggplot(taxa_meta_stage, aes(x = habitatName, fill = habitatName))+
  geom_bar(stat = "count", color = "Black")+
  theme_classic()+
  labs(x = "Habitat Type", y = "No. of Observations")+
  theme(legend.position = "none")
```

Almost all targeted riffle, few "Riffle"

##### iv. Mesh Size - Keep all for now:

```{r}
ggplot(taxa_meta_stage, aes(x = factor(mesh), fill = factor(mesh)))+
  geom_bar(stat = "count", color = "Black")+
  theme_classic()+
  labs(x = "Mesh Size", y = "No. of Observations")+
  theme(legend.position = "none")
```

Quite a bit of variation, also lots of NAs. 


#### C. Classification Level - Drop everything not classified to Genus: 

###### Aside: Extract Chironomidae data before filtering by classification level

Extract all data in family Chironomidae; Also apply sample date (>1990) and area (>0.05 & <5) filters from below:

```{r}
chiro <- taxa_meta_stage%>%
  filter(family=="Chironomidae")%>% #Extract all rows with family = Chironomidae
  filter(!area<0.05, !area>5)%>% #Drop samples with very large and very small areas
  filter(!year(sampleDate)<1990)%>% #Drop samples pre-1990
  mutate(density = (splitCount*sampleProp)/area)
```

Save:

```{r}
write.csv(chiro, "cleaned_data/chironomidae.csv")
```

##### i. How much data is classified to genus?

First - check which codes correspond with which lifestage

```{r}
level_codes <- taxa_meta %>%
  select(levelName, levelId)%>% #extract level names and codes
  distinct()%>% #combine duplicate rows
  arrange(levelId) #sort by levelID
level_codes
```

Visual: 

```{r}
level.hist <- ggplot(taxa_meta_stage, aes(x = levelId))+ #x axis = levelId
  geom_histogram(fill = "Grey", color = "Black", bins = 15)+ #changing histogram color and bin size
  geom_vline(xintercept = 23, linetype = "dashed", color = "Red")+ #adding vertical line at 23 (code for genus)
  theme_classic()+
  labs( x = "Taxonomic Level ID", y = "Count") #axis labels
level.hist
```

Dashed red line = Genus - all samples with smaller taxonomic ID's are classified to higher levels. 

If we just used observations classified to genus or lower, we would lose `r round((nrow(filter(taxa_meta_labsplit, levelId < 23)) / nrow(taxa_meta_labsplit))*100, 2)` percent of the data, leaving  `r nrow(filter(taxa_meta_labsplit, levelId >= 23))` observations remaining. 

##### ii. Dropping all individuals not ID'd to at least genus

```{r}
taxa_meta_genus <- taxa_meta_stage %>%
  filter(!levelId<23) #dropping all rows with level ID <23 (23 and up = genus or finer)
```

What effect does this have on the data? 

```{r}
#No samples with >300 individuals after dropping:
nrow(taxa_meta_genus %>%
  group_by(sampleId)%>%
  summarise(n_indiv = sum(splitCount))%>%
  filter(n_indiv>=300))

#No samples with >300 before dropping
nrow(taxa_meta_stage %>%
  group_by(sampleId)%>%
  summarise(n_indiv = sum(splitCount))%>%
  filter(n_indiv>=300))

#% retained
(nrow(taxa_meta_genus %>%
  group_by(sampleId)%>%
  summarise(n_indiv = sum(splitCount))%>%
  filter(n_indiv>=300))/nrow(taxa_meta_stage %>%
  group_by(sampleId)%>%
  summarise(n_indiv = sum(splitCount))%>%
  filter(n_indiv>=300))
)*100
```

Loose ~39% of the data when dropping individuals not ID'd to genus and samples with <300 individuals. 


```{r}
#After dropping above genus
taxa_meta_genus %>%
  group_by(sampleId)%>%
  summarise(n_indiv = sum(splitCount))%>%
  ggplot(aes(x = n_indiv))+
  geom_histogram(fill = "grey", color = "black", bins = 70)+
  labs(title = "After dropping samples not ID'd to genus", x = "# Individuals in sample", y = "# of samples")+
  theme_classic()+
  geom_vline(xintercept = 300, size = 0.5, linetype = "dashed", color = "red")+
  geom_label(aes(x = 350, y = 600, label = "2901 samples \n with >300 indiv."), fill = "red", fontface = "bold", hjust = 0)

#before dropping above genus
taxa_meta_stage %>%
  group_by(sampleId)%>%
  summarise(n_indiv = sum(splitCount))%>%
  ggplot(aes(x = n_indiv))+
  geom_histogram(fill = "grey", color = "black", bins = 70)+
  labs(title = "Before dropping samples not ID'd to genus", x = "# Individuals in sample", y = "# of samples")+
  theme_classic()+
  geom_vline(xintercept = 300, size = 0.5, linetype = "dashed", color = "red")+
    geom_label(aes(x = 350, y = 800, label = "4716 samples \n with >300 indiv."), fill = "red", fontface = "bold", hjust = 0)

```


#### D. Checking area ranges and applying area filters:

First pass - basic summary of sample area: 

```{r}
summary(metadata$area)
```

26 samples missing area, some samples with area of 100? 

Visual check: 

```{r}
ggplot(metadata, aes(x = area))+
  geom_histogram(color = "Black", fill = "Grey", bins = 70)+ 
  theme_classic()+
  labs(x = "Sample Area, m2", y = "Count")
```

Six samples have area = 100 - visualization without those six samples:

```{r}
ggplot(filter(metadata, area <50), aes(x = area))+
  geom_histogram(color = "Black", fill = "Grey", bins = 70)+ 
  theme_classic()+
  labs(x = "Sample Area, m2", y = "Count")+
  geom_vline(aes(xintercept = 0.05), color = "red", size = 0.5, linetype = "dashed")+
  geom_vline(aes(xintercept = 5), color = "red", size = 0.5, linetype = "dashed")
```

Much more believable. 

**Final Filter for now:** Removing samples with no area,  with area< 0.05m & >5m from the dataset (same cutoffs used by Rumschlag et al: 

```{r}
taxa_meta_area <- taxa_meta_genus%>%
  filter(!is.na(area), 
         !area <0.05, 
         !area >5)
```

How many samples with area > "x":

```{r}
n<- nrow(metadata)

ggplot(filter(metadata, area<50), aes(x = area))+
  stat_ecdf(aes(y = (1-after_stat(y))), geom = "line", size = 1, color = "red")+
  scale_y_continuous("Proportion of samples w/ Area > x", position = "left", sec.axis = sec_axis(name = "# Samples w/ Area > x", trans = ~.x*n))+
  theme_linedraw()+
  labs(x ="Sample Area")+
  geom_vline(xintercept = 5, linetype = "dashed")+
  geom_vline(xintercept = 0.05, linetype = "dashed")
```

#### E. Checking sample date ranges and applying filters:

First, visualization of sample dates:

```{r}
ggplot(metadata, aes(x = year(sampleDate)))+
  geom_histogram(color = "black", fill = "grey", bins = 70)+
  theme_classic()
```

A few samples from before 1990, vast majority post-1990. 

**Rule for now:** Drop all samples pre-1990:

```{r}
taxa_meta_date <- taxa_meta_area %>%
  filter(!year(sampleDate)<1990)
```

#### F. Calculate Density: 

Calculate density for each observation - density = (# organisms/(labSplit*fieldSplit))/area sampled

```{r}
taxa_meta_density <- taxa_meta_date %>%
  mutate(taxa_density = (splitCount/(labSplit*fieldSplit))/area)

dim(taxa_meta_density)
```

Add site elevations: 

```{r}
tmd_sites <- taxa_meta_density%>%
  select(siteId, siteLongitude, siteLatitude)%>%
  distinct()

elev1 <- elevation(latitude = tmd_sites$siteLatitude, longitude = tmd_sites$siteLongitude, elevation_model = "srtm3", username = "ggianniny")%>% #calculate elevation using the SRTM3 model (uses GeoNames API)
  mutate(siteId = tmd_sites$siteId)%>% #add siteIDs 
  rename(elev_m = elevation_geonames)%>% #rename cols for merge
  select(siteId, elev_m)%>% #drop extra cols for merge
  distinct() #remove duplicates

taxa_meta_density2 <- merge(taxa_meta_density, elev1) #merge with taxonomic data
```

Save this file for future analysis: 

```{r}
write.csv(taxa_meta_density2, "cleaned_data/taxa_densities.csv")
```

### 3. Calculate Richness & Deal with Replicates

Assumptions for richness calculations are: 

  * We are only including larvae, pupae, and adults. 
  
  * We can treat all three sampling methods and habitat types as equivalent
  
  * All of the unique values of "scientificName" represent unique taxa (probably not true, but sticking with that for now)
  
  * We don't need to make any adjustments for area sampled or lab/field split ratios. 
  
  * We don't need to make any adjustments for variation in mesh size. 

#### A. Calculate Richness for each sample:

First, calculate species richness and total abundance for each sample: 

```{r}
sample_richness <- taxa_meta_density %>%
  group_by(sampleId, labSplit, fieldSplit, siteId, siteLongitude, siteLatitude, sampleMethod,habitatName, area, mesh, COMID, sampleDate)%>% #group by sampleId, plus keep other cols for future use. 
  summarise(richness = length(unique(scientificName)), #calculate richness = # unique values of scientificName in each sampleID
            tot_abundance = sum(splitCount), #calculate total abundance = sum of counts for all taxa in that sample
            tot_dens = sum(taxa_density)) #calculate total density = sum of density (individuals/m2) for all taxa in that sample
```
    
Save this file for future analyses: 

```{r}
write.csv(sample_richness, "cleaned_data/sample_richness.csv")
```

#### B. Average true replicates (samples from the same day or within one day of eachother).

Average all samples collected from the same site on the same day: 

```{r}
site_richness_day <- sample_richness %>%
  group_by(siteId, sampleDate, siteLongitude, siteLatitude, habitatName, area, mesh, COMID)%>% # group by siteId and sampleDate
  summarise(richness_xbar = mean(richness), #calculate mean richness
            abund_xbar = mean(tot_abundance), #calculate mean abundance
            density_xbar = mean(tot_dens), #calculate mean density
            labSplit = mean(labSplit), #mean labSplit across replicates
            fieldSplit = mean(fieldSplit) #mean fieldSplit across replicates
            #,sampleMethod = unique(sampleMethod) NOT RUN - keeping sampleMethod (results in keeping some samples seperate)
            ) 
```

Average all samples from the same site within **5 days** of each other: *(NOTE: haven't been able to figure out how to merge obs with in 1 day of eachother)*

```{r}
site_richness_5d <- site_richness_day %>%
  arrange(siteId, sampleDate)%>% #sort by siteId and sampleDate
  mutate(doy = yday(sampleDate), #new column with day of year of sample
         doy_round = (5*round(doy/5)), #round day of year to nearest 5
         yr = year(sampleDate), #new column with year of sample
         site_doy_yr = paste(siteId, doy_round, yr, sep = "_"))%>% #new column with combined string of siteId, rounded day of year, and sample year for grouping
  group_by(site_doy_yr, siteId, siteLongitude, siteLatitude, habitatName, area, mesh, COMID)%>% #group by siteId/doy/yr column, retain other cols
  summarise(richness_xbar = mean(richness_xbar), #calculate mean richness
            abund_xbar = mean(abund_xbar), #calculate mean abundance
             density_xbar = mean(density_xbar), #calculate mean density
            labSplit = mean(labSplit), #calculate mean labSplit
            fieldSplit = mean(fieldSplit), #calculate mean fieldSplit
            sampleDate = mean(sampleDate)) #calculate mean sampleDate

```

Check which rows were combined:

```{r}
sr5_comp <- site_richness_5d[,2:14] #get rid of extra column in new df for comparison
combined <- dplyr::setdiff(site_richness_day, sr5_comp) #new object with the 10 rows that are different between the daily dataframe and the 5 day dataframe
combined
```

All rows in the daily dataset that aren't in the 5-day dataset have sampleDates within 5 days of eachother, so appears to be working correctly. 

Save this file: 

```{r}
write.csv(sr5_comp, "cleaned_data/site_5day_richness.csv")
```

#### C. Average within-year samples

Calclulate averages:

```{r}
site_yr_richness <- sr5_comp %>%
  mutate(yr = year(sampleDate))%>% #add column for year
  group_by(siteId, yr)%>% #group by site and year
  summarise(richness_xbar = mean(richness_xbar), #calculate mean richness, abundance, lab/fieldsplit, area, and mesh. 
            abund_xbar = mean(abund_xbar), 
            density_xbar = mean(density_xbar), #calculate mean density
            labSplit = mean(labSplit), 
            fieldSplit = mean(fieldSplit), 
            area = mean(area), 
            mesh = mean(mesh, na.rm = T))
```

Add metadata back in:

```{r}
meta_sub <- metadata%>%
  select(siteId, siteLatitude, siteLongitude, COMID)%>% #select desired metadata cols
  distinct()

site_yr_richness2 <- merge(site_yr_richness, meta_sub) #merge with site x year richness data
```

Save file: 

```{r}
write.csv(site_yr_richness2, "cleaned_data/site_year_richness.csv") 
```

#### D. Average within-reach (same COMID) samples

Calculate average richness and abundance within each reach:

```{r}
comid_yr_richness <- site_yr_richness2 %>%
  group_by(COMID, yr)%>% #group by COMID, year
  summarise(richness_xbar = mean(richness_xbar), #calculate mean richnes, abundance, labSplit, fieldSplit, area, and mesh w/in each COMID * year combo. 
            abund_xbar = mean(abund_xbar),             
            density_xbar = mean(density_xbar), #calculate mean density
            labSplit = mean(labSplit), 
            fieldSplit = mean(fieldSplit), 
            area = mean(area), 
            mesh = mean(mesh, na.rm = T))
```

Save: 

```{r}
write.csv(comid_yr_richness, "cleaned_data/comid_year_richness.csv")
```

### 4. ID and seperate sites with longer records - minimum... 4 years of data?

#### A. By site: 

Calculate number of years of data at each site: 

```{r}
site_pors <- site_yr_richness2 %>%
  group_by(siteId)%>% #group by siteId
  summarise(por = length(unique(yr)))%>% #count # years data at that site
  arrange(desc(por)) #descending sort by period of record
```

Visualization: how many sites w/ longer-term datasets?  

```{r}
ggplot(site_pors, aes(x = factor(por), fill = factor(por)))+
  geom_bar(stat = "count", color = "Black")+
  theme_classic()+
  labs(x = "No. years of data", y = "No. sites")+
  theme(legend.position = "none")
```

Lots of sites have only 1 or 2 years of data - if use minimum 4 years, this would leave `r nrow(filter(site_pors, por >=4))` sites. 

**Extract subset of data w/ at least 4 years of data**: 

Extract site Id's for all sites with 4+ years of data:

```{r}
sites.4plus <- site_pors %>%
  filter(por >= 4)%>% #remove all sites with <4yrs data
  select(siteId) #drop extra column
```

Filter site * year richness data to only include these sites; add elevation data:  

```{r}
site_yr_longterm <- site_yr_richness2%>%
  filter(siteId%in%sites.4plus$siteId)#filter to drop all siteId's not in the sites.4plus list

elev <- elevation(latitude = site_yr_longterm$siteLatitude, longitude = site_yr_longterm$siteLongitude, elevation_model = "srtm3", username = "ggianniny")%>% #calculate elevation using the SRTM3 model (uses GeoNames API)
  mutate(siteId = site_yr_longterm$siteId)%>% #add siteIDs 
  rename(elev_m = elevation_geonames)%>% #rename cols for merge
  select(siteId, elev_m)%>% #drop extra cols for merge
  distinct() #remove duplicates

site_yr_longterm2 <- merge(site_yr_longterm, elev) #merge with taxonomic data
```

Save: 

```{r}
write.csv(site_yr_longterm2, "cleaned_data/longterm_site_richness.csv")
```

#### B. By COMID: 

Calculate number of years of data for each COMID: 

```{r}
comid_pors <- comid_yr_richness %>%
  group_by(COMID)%>% #group by COMID
  summarise(por = length(unique(yr)))%>% #count # years data at that COMID
  arrange(desc(por)) #descending sort by period of record
```

Visualization: how many sites w/ longer-term datasets?  

```{r}
ggplot(comid_pors, aes(x = factor(por), fill = factor(por)))+
  geom_bar(stat = "count", color = "Black")+
  theme_classic()+
  labs(x = "No. years of data", y = "No. COMIDs")+
  theme(legend.position = "none")
```

Lots of sites have only 1 or 2 years of data - if use minimum 4 years, this would leave `r nrow(filter(comid_pors, por >=4))` sites - so this adds an extra `r (nrow(filter(comid_pors, por >=4)))-(nrow(sites.4plus))` COMID's as compared to using the site-level data. 

**Extract subset of data w/ at least 4 years of data**: 

Extract site Id's for all sites with 4+ years of data:

```{r}
comid.4plus <- comid_pors %>%
  filter(por >= 4)%>% #remove all sites with <4yrs data
  select(COMID) #drop extra column
```

Filter site * year richness data to only include these sites: 

```{r}
comid_yr_longterm <- comid_yr_richness%>%
  filter(COMID%in%comid.4plus$COMID) #filter to drop all COMID's not in the comid.4plus list
```

Save: 

```{r}
write.csv(site_yr_longterm, "cleaned_data/longterm_comid_richness.csv")
```

### 5. Identify species of concern; create new dataframe with all SGCNs from UT and surrounding states:

#### A. UT SSGCNs:

Make a list of all invertebrate UT SGCNs (from https://wildlife.utah.gov/pdf/WAP/2022-05-sgcn-list.pdf):

```{r}
ut.sgcn.list <- c( "Pacifastacus gambelii", "Stygobromus utahensis","Oreohelix yavapai cummingsi", "Fossaria techella","Pyrgulopsis pilsbryana", "Pyrgulopsis peculiaris", "Pyrgulopsis plicata", "Oreohelix parawanensis", "Pyrgulopsis inopinata", "Physa megalochlamys", "Planorbella binneyi", "Gastrocopta quadridens", "Oreohelix peripherica", "Pyrgulopsis deserta", "Tryonia porrecta", "Oreohelix eurekensis", "Stagnicola bonnevillensis", "Fluminicola coloradoensis", "Pyrgulopsis hamlinensis", "Oxyloma kanabense", "Planorbella oregonensis ", "Pyrgulopsis lindahlae", "Pyrgulopsis anguina"," Oreohelix haydeni", "Oreohelix howardi","Vertigo concinnula", "Gastrocopta pilsbryana", "Stagnicola montanensis", "Pyrgulopsis variegata", "Pyrgulopsis nuwuvi", "Pyrgulopsis fusca", "Pyrgulopsis pinetorum", "Pupoides hordaceus", "Colligyrus greggi", "Succinea rusticana", "Pyrgulopsis santaclarensis", "Catinella stretchiana", "Gastrocopta ashmuni", "Pyrgulopsis chamberlini", "Ogaridiscus subrupicola", "Hawaiia neomexicana", "Pyrgulopsis saxatilis", "Vallonia perspectiva", "Pupilla syngenes","Physella utahensis", "Margaritifera falcata", "Physella zionis", "Stagnicola traski", "Anodonta nutalliana") 

ut_sgcn <- taxa_meta_density2 %>%
 filter(scientificName%in%ut.sgcn.list)

ut_sgcn<- ut_sgcn%>%
  mutate(state = rep("UT", nrow(ut_sgcn)))

sort(unique(ut_sgcn$scientificName)) #return list of species present in this dataset. 
```

Four UT SGCNs present in the dataset. 

#### B. Idaho SGCNs:

Listed at https://www.landcan.org/pdfs/appendixB.pdf  

Make a vector with all ID SGCN scientific names; extract data with those taxa present: 

```{r}
id.sgcn.list <- c("Caurinella idahoensis","Ametropus ammophilus", "Centroptilum selanderorum", "Capnia zukeli", "Soyedina potteri", "Pictetiella expansa", "Agapetus montanus", "Anodonta californiensis", "Gonidea angulata", "Margaritifera falcata", "Oreohelix waltoni", "Oreohelix vortex", "Oreohelix strigosa goniogyra", "Oreohelix jugalis", "Oreohelix intersum", "Oreohelix idahoensis", "Oreohelix haydeni", "Polygyrella polygyrella", "Cryptomastix populi", "Cryptomastix harfordiana", "Cryptomastix mullani tuckeri", "Cryptomastix mullani latilabris", "Cryptomastix mullani clappi", "Cryptomastix mullani blandi", "Cryptomastix magnidentata", "Allogona ptychophora solida", "Allogona lombardii", "Pristiloma idahoense", "Udosarx lyrata", "Zacoleus idahoensis", "Prophysaon humile", "Hemphillia camelus", "Kootenaia burkei", " Discus marmorensis", "Anguispira nimapuna", "Helicodiscus salmonaceus", "Radiodiscus abietum", "Planogyra clappi", "Physa natricina", "Lanx", "Fisherola nuttalli", "Stagnicola montanensis", "Stagnicola idahoensis", "Stagnicola hinkleyi", "Valvata utahensis", "Taylorconcha serpenticola", "Pristinicola hemphilli", "Pyrgulopsis robusta", "Pyrgulopsis pilsbryana", "Pyrgulopsis bruneauensis", "Fluminicola coloradoensis")

id_sgcn <- taxa_meta_density2%>%
  filter(scientificName%in%id.sgcn.list)
  
id_sgcn<- id_sgcn%>%
  mutate(state = rep("ID", nrow(id_sgcn)))

sort(unique(id_sgcn$scientificName))
```

5 Idaho SGCNs present in the dataset. 


#### C. Wyoming SGCNs:

List of all invertebrate SGCNs downloaded from:

https://wyndd.org/species_list/?wgfd_sgcn=Statewide&wy_occur=Regular%2CIrregular&wy_origin=Native%2CNonnative&res_status=Current&tax_level=Species%2CSubspecies%2FVariety&columns=sciname%2Ccomname%2Csynonyms%2Ctaxgroup%2Ctaxgroup2%2Cblm_wy%2Cg_rank%2Cs_rank%2Cusfws_esa%2Cusfs_sens%2Cusfs_solc%2Cwgfd_sgcn%2Cwy_contrib%2Cwyndd_soc%2Cwy_occur%2Cwy_origin&loc_settings=documentedObs%2Cdist_mod%2Cregular%2Cirregular&servicePrefix=prod_ 

Read in list of all inverts:

```{r}
wy.sgcn.list <- read.csv("Wyoming_SGCNS.csv", skip = 1)
```

Make new dataframe with all present in UT: 

```{r}
wy_sgcn <- taxa_meta_density2 %>%
  filter(scientificName%in%wy.sgcn.list$Scientific.Name)

wy_sgcn <- wy_sgcn%>%
  mutate(state = rep("WY", nrow(wy_sgcn)))

sort(unique(wy_sgcn$scientificName))
```

11 Wyoming SGCNs present in the dataset

#### D. Colorado SGCNs: 

Make list of Colorado invertebrate SGCNs; filter out of full dataset: 

Mollusks from https://cpw.state.co.us/Documents/WildlifeSpecies/SWAP/CO_SWAP_Chapter2.pdf 
Invertebrates from https://cpw.state.co.us/Documents/WildlifeSpecies/SWAP/CO_SWAP_FULLVERSION.pdf (Appendix B, table B-1 starting on page B-1)

```{r}
co.sgcn.list <- c("Ferrissia walker",
"Promenetus umbillicatellus", "Anodontoides ferussacianus", "Ferrissia fragilis",
"Physa cupreonitens", "Uniomerus tetralasmus", "Acroloxus coloradensis", "Promenetus exacuous", "Physa gyrina utahensis", "Gumaga griseola", "Ochrotrichia susanae", "Ochrotrichia trapoiza", "Argia alberta", "Calopteryx aequabilis", "Calopteryx maculata", "Cordulegaster dorsalis", "Dromogomphus spoliatus", "Enallagma vesperum", "Epitheca petechialis", "Erpetogomphus compositus", "Erythemis vesiculosa", "Lestes alacer","Leucorrhinia glacialis", "Libellula nodisticta", "Somatochlora ensigera", "Somatochlora hudsonica", "Stylurus intricatus", "Sympetrum madidum","Acentrella parvula","Acerpenna pygmaea", "Ametropus neavei","Apobaetis etowah","Baetis brunneicolor", "Camelobaetidius warreni", "Ephemerella apopsis", "Homoeneuria alleni", "Labiobaetis apache", "Lachlania saskatchewanensis", "Macdunnoa persimplex", "Neochoroterpes oklahoma", "Plauditus cestus","Pseudiron centralis", "Rhithrogena flavianula","Arsapnia arapahoe", "Capnia arapahoe", "Capnia nelsoni","Mesocapnia frisoni", "Pteronarcys californica", "Nemomydas solitarius")

co_sgcn <- taxa_meta_density2%>%
  filter(scientificName%in%co.sgcn.list)

co_sgcn <- co_sgcn%>%
  mutate(state = rep("CO", nrow(co_sgcn)))

sort(unique(co_sgcn$scientificName))
```

9 Colorado SGCNs present in the dataset. 

#### E. Arizona SGCNs

Full list downloaded from: https://awcs.azgfd.com/appendices/appendix-d-species-of-greatest-conservation-need-with-vulnerability-scores 

Read in datafile with all AZ SGCNs, extract invertebrates: 

```{r}
az.sgcn.list <- read.csv("AZ_SGCNs.csv")%>%
  filter(Taxonomic.Group == "Invertebrate")
```

Filter out observations of these taxa from the full dataset: 

```{r}
az_sgcn <- taxa_meta_density2%>%
  filter(scientificName%in%az.sgcn.list$Scientific.Name)

az_sgcn<- az_sgcn%>%
  mutate(state = rep("AZ", nrow(az_sgcn)))

sort(unique(az_sgcn$scientificName))
```

5 AZ SGCNs in the dataset. 

#### F. Nevada SGCNs: 

Make a list of all Nevada invertebrate SGCNs (from https://www.ndow.org/wp-content/uploads/2022/01/2013-NV-WAP-Complete-NOT-ADA.pdf, table starting on page 73)

```{r}
nv.sgcn.list <- c("Anodonta californiensis","Tryonia variegata", "Pyrgulopsis pellita", "Pyrgulopsis erythropoma", "Pyrgulopsis peculiaris", "Pyrgulopsis papillata", "Pyrgulopsis coloradensis", "Pyrgulopsis lata", "Pyrgulopsis montana", "Pyrgulopsis fausta", "Pyrgulopsis crystalis", "Pyrgulopsis nanus", "Pyrgulopsis dixensis", "Pyrgulopsis aloba", "Pyrgulopsis villacampae", "Pyrgulopsis leporina", "Pyrgulopsis augustae", "Pyrgulopsis notidicola", "Pyrgulopsis isolata", "Pyrgulopsis gracilis", "Pyrgulopsis fairbanksensis", "Pyrgulopsis breviloba", "Pyrgulopsis planulata", "Pyrgulopsis bruesi", "Tryonia clathrata", "Pyrgulopsis marcida", "Pyrgulopsis hubbsi", "Pyrgulopsis humboldtensis", "Pyrgulopsis sathos", "Pyrgulopsis wongi","Pyrgulopsis imperialis", "Pyrgulopsis sublata", "Pyrgulopsis landyei", "Pyrgulopsis basiglans", "Pyrgulopsis lockensis", "Pyrgulopsis anguina", "Pyrgulopsis pisteri", "Tryonia ericae", "Pyrgulopsis avernalis", "Pyrgulopsis carinifera", "Tryonia monitorae", "Pyrgulopsis neritella", "Pyrgulopsis militaris", "Pyrgulopsis serrata", "Pyrgulopsis variegata", "Pyrgulopsis micrococcus", "Pyrgulopsis pictilis", "Pyrgulopsis merriami", "Pyrgulopsis aurata", "Tryonia elata","Fluminicola dalli", "Pyrgulopsis sadai", "Pyrgulopsis bifurcata", "Juga interioris", "Pyrgulopsis turbatrix", "Pyrgulopsis anatina", "Pyrgulopsis umbilicata", "Pyrgulopsis sulcata", "Tryonia angulata", "Pyrgulopsis deaconi", "Pyrgulopsis limaria", "Eremopyrgus eganensis", "Pyrgulopsis sterilis", "Pyrgulopsis orbiculata", "Pyrgulopsis cruciglans", "Fluminicola turbiniformis", "Pyrgulopsis millenaria", "Pyrgulopsis hovinghi", "Pyrgulopsis vinyardi", "Fluminicola virginius")

nv_sgcn <- taxa_meta_density2 %>%
  filter(scientificName%in%nv.sgcn.list)

nv_sgcn<- nv_sgcn%>%
  mutate(state = rep("NV", nrow(nv_sgcn)))

sort(unique(nv_sgcn$scientificName))
```

Only 1 NV SGCN present in dataset. 

#### G. Combine all states and save: 

```{r}
all_sgcn <- rbind(ut_sgcn, id_sgcn, wy_sgcn, co_sgcn, az_sgcn, nv_sgcn) #rowbind data from all 6 states


#Combine states for taxa that are listed in multiple states (e.g. instead of a row for Taxa 1/ UT and another row for Taxa 1/ NV, want 1 row with Taxa 1 / UT,NV)

sgcn_states <- all_sgcn%>% 
  distinct(scientificName, state)%>% #extract states and taxa names
  arrange(scientificName)%>% #sort by taxa name
  group_by(scientificName)%>% # group by taxa name
  summarise(states = toString(unique(state))) #create new "states" column with a character string of all unique values of "state" within that taxa

all_sgcn_states <- merge(all_sgcn, sgcn_states)%>% #merge with the full SGCN dataset
  select(!state)%>% #drop the old "state" column
  filter(!splitCount == 0)%>% #drop taxa with split counts of 0
  distinct() #drop duplicate rows
  
sort(unique(all_sgcn_states$scientificName)) #return list of all SGCN taxa present in the data
```

Total of 20 SGCNs that are listed in different states. **NOTE - some of these taxa (4) are listed in the dataset but have splitCounts of 0 - dropping those here. **

Save this file for future analysis: 

```{r}
write.csv(all_sgcn_states,"cleaned_data/sgcns.csv")
```

### 6. Adding Utah Ecoregion Data; Calculating Ecoregion-level diversity metrics:

First, read in shapefile of UT ecoregions and extract site coordinates:

```{r}
ut_er <- readOGR("gis/data/ut_eco_l3")%>% #read in shapefile
  spTransform(("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")) #Update CRS
  

site_xy <- metadata %>%
  select(siteId, siteLongitude, siteLatitude) #extract site coords
```

Convert site coordinates to spatial coordinates: 

```{r}
xy <- site_xy[,c(2,3)] #extract lat/long cols

spdf <- SpatialPointsDataFrame(coords = xy, data = site_xy,#create spatialPoints DF using lat/long cols
                               proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")) #updating projection
```

Intersect points with shapefile: 

```{r}
site_ers <- over(spdf, ut_er)%>%
  bind_cols(site_xy) #add site coordinates back to dataframe
```

Check with plotting: 

```{r}
ut_er@data$id <- rownames(ut_er@data) #add ID column for join

ut_er_df <- fortify(ut_er)%>% #transform ecoregion spatialPolygons Dataframe into a dataframe
  left_join(ut_er@data, by = "id") #add ecoregion data back in

ggplot(ut_er_df, aes(x=long, y = lat, group = group, fill = US_L3NAME))+ #plot ecoregions, fill based on region
  geom_polygon(color = "black"
               , alpha = 0.25
               )+ #polygon aesthetics - adjusting line colors and transparency
  theme_void()+
 geom_point(data = site_ers, aes(x = siteLongitude, y = siteLatitude, group = NULL, color = US_L3NAME, fill = NULL), size = 0.5)+ #add points for each site, color-coded by ecoregion
  labs(color = "Ecoregion Name", fill = "Ecoregion Name") #updating legend title
```

Save plot (w/ points):

```{r}
ggsave("plots/ut_ecoregions.pdf", plot = last_plot(), device = "pdf", units = "in", width = 6.5, height = 5.5, dpi = "retina")
```



Success! Now, extract ecoregion info and add back to primary dataset: 

```{r}
ecoregions <- site_ers %>%
  select(siteId, US_L3CODE, US_L3NAME)%>%
  rename(ecoCode = US_L3CODE, ecoName = US_L3NAME)%>% #extract ecoregion ID and name
  distinct()

#Save for future use:
write.csv(ecoregions,"cleaned_data/site_ecoregions.csv")
```

Add to richness dataset: 

```{r}
richness_eco <- merge(site_yr_richness2, ecoregions)
#Save: 
write.csv(richness_eco, "cleaned_data/eco_richness.csv")
```

Add to density dataset: 

```{r}
density_eco <- merge(taxa_meta_density2, ecoregions)
#Save: 
write.csv(density_eco, "cleaned_data/eco_density.csv")
```

#### A. Data Cleaning

##### i. Drop samples with <300 individuals; subsample those with >300 individuals to 300 individuals: 

First, add identify samples with <300 observations: 

```{r}
under300 <- density_eco %>%
  group_by(sampleId)%>% #group by sampleId
  summarise(n_indiv = sum(splitCount))%>% #count total individuals in sample
  filter(n_indiv < 300)%>% #extract samples with fewer than 300 individuals
  distinct() #remove duplicate rows
```

Drop these samples from the dataset; calculate a total density metric before subsampling to 300 individuals:

```{r}
density_eco_300 <- density_eco %>%
  filter(!sampleId%in%under300$sampleId)%>% #drop sample ID's in list created above
  group_by(sampleId)%>% #group by sample ID
  mutate(tot_dens = sum(taxa_density))%>% #calculate total density in each sample
  ungroup() #ungroup to retain all cols
head(density_eco_300)
```

Randomly subsample the remaining samples to 300 individuals: 

First, make new dataframe with a row for each individual in each sample (i.e. for sample A with 6 observations of organism B, want 6 rows with "organism B"): 

```{r}
sampleId.reps <- rep(density_eco_300$sampleId, density_eco_300$splitCount) #repeat sample ID for # in splitCount col
taxaName.reps <- rep(density_eco_300$scientificName, density_eco_300$splitCount) #repeat taxa name for # in splitCount col
taxa_counts <- data.frame(sampleId.reps, taxaName.reps) #create new dataframe with these objects
```

Randomly select 300 observations from within each sample ID: 

```{r}
set.seed(123) #for reproducibility

sample_subsets <- taxa_counts %>%
  group_by(sampleId.reps)%>% #group by sample ID
  slice_sample(n = 300)%>% #select 300 random rows from within that sample. 
  arrange(sampleId.reps, taxaName.reps)

nrow(sample_subsets)/length(unique(sample_subsets$sampleId.reps)) #double checking that correct no. rows were selected
```

Distill this dataframe back to 1 row for each sample + taxa combo, column for splitCount for that taxa

```{r}
subsets_short <- sample_subsets %>%
  group_by_all()%>% #group by all columns
  count()%>% #count # duplicate columns and add to new col
  rename(splitCount = n, sampleId = sampleId.reps, scientificName = taxaName.reps)  #updating column names for merge
```

Add sample metadata back in: 

```{r}
eco_meta <- density_eco_300%>%
  select(1:4, 21:38, -taxa_density)%>% #select metadata columns from ecoregion density DF (including total sample density calculated before sub-sampling)
  distinct() #remove duplicate rows

subsets_eco <- merge(subsets_short, eco_meta)%>% #merge with subsetted taxonomic data
  mutate(yr = year(sampleDate)) #add year column
```

Add taxonomic information back in:

```{r}
taxa_info <- density_eco_300 %>%
  select(taxonomyId, scientificName, phylum, class, order, family, subFamily, genus, species)%>%
  distinct()

subsets_eco1 <- merge(subsets_eco, taxa_info, all.y = F)
```


Save this file for future analyses:

```{r}
write.csv(subsets_eco1, "cleaned_data/subsetted_full.csv")
```


##### ii. Drop years with only a few sites: 

**NOTE - for now, using minimum cutoff of 10 sites for each ecoregion*year combo - can adjust as needed.**

Change cutoff value here:

```{r}
site.cutoff <- 9
```

First, check how many ecoregion * year combos have fewer sites than the cutoff: 

```{r}
sites_perYr <- subsets_eco1 %>%
  group_by(ecoName, yr)%>% #group by ecoregion and year
  summarise(n_sites = length(unique(siteId)))%>% #count n sites in each ecoregion in each year
  filter(n_sites < site.cutoff)%>% #filter out rows w fewer sites than threshold specified above
  mutate(to_drop =paste(ecoName, yr, sep = " ")) #create combined ecoregion+year column 
```

`r nrow(sites_perYr)` ecoregion * year combos with fewer sites than the threshold.

Drop ecoregion * yr combos below the cutoff: 

```{r}
subsets_eco2 <- subsets_eco1 %>%
  mutate(eco_yr = paste(ecoName, yr, sep = " "))%>% #add ecoregion+year column 
  filter(!eco_yr%in%sites_perYr$to_drop)%>%#drop ecoregion+year combos in sites_perYr
  select(!eco_yr) #remove extra ecoregion+year column 

unique(subsets_eco1$ecoName)
```


##### iii. Extract primary ecoregions: Wasatch/Uinta, Co Plateau, Central Basin & Range:

Extract data for three primary UT Ecoregions: 

```{r}
subsets_eco3 <- subsets_eco2 %>%
  filter(ecoCode == 19|ecoCode == 20|ecoCode == 13) #Extract rows with eco codes corresponding to 3 primary UT ecoregions
```

##### iv. Plotting to check for undue influence of individual sites/years: 

Check for outlier years in density/richness by plotting richness/density for each site over time, faceted by ecoregion. 

Calculate richness and total density for each sample: 

```{r}
ecoSite_means <- subsets_eco3 %>%
  group_by(sampleId)%>% #group by sample ID
  mutate(tot_dens = unique(tot_dens), 
         richness = length(unique(scientificName)))%>% #calculate total density and richness for each sample
  ungroup()
  
```


On to plotting:

**Density:**

```{r}
dens.eco.site <- ggplot(ecoSite_means, aes(x = yr, y = log10(tot_dens), group = factor(siteId), color = ecoName))+
  geom_point()+
  geom_line()+
  facet_wrap(~ecoName)+
  theme_classic()+
  theme(legend.position = "none")+
  labs(x = "", y = "log10(Mean density, individuals/m2)")
dens.eco.site
```

**Richness:**

```{r}
rich.eco.site <- ggplot(ecoSite_means, aes(x = yr, y = log10(richness), group = factor(siteId), color = ecoName))+
  geom_point()+
  geom_line()+
  facet_wrap(~ecoName)+
  theme_classic()+
  theme(legend.position = "none")+
  labs(x = "Year", y = "log10(Mean Taxonomic Richness)")
rich.eco.site
```

Combine plots and save:

```{r}
eco.site.comb <- dens.eco.site/rich.eco.site
eco.site.comb

#save:
ggsave("plots/ecoSite_richness_density.jpg", eco.site.comb, device = "jpeg", units = "in", height = 9, width = 6.5, dpi = "retina")
```

#### B. Calculate ecoregion-level richness and density for each year: 

Count # unique species and calculate avg. total sample density in each ecoregion in each year: 

NOT RUN
eco_r_d <- subsets_eco2 %>%
  group_by(ecoName, yr)%>% #group by ecoregion name and year
  summarise(ecoCode = unique(ecoCode), #add ecoregion code back in
            richness = length(unique(scientificName)), #count no unique taxa
            density_xbar = mean(tot_dens), #calculate mean density
            elev_xbar = mean(elev_m), #calculate mean sample elevation
            n_samples = length(unique(sampleId)))%>% #count number of samples per year*ecoregion
  mutate(richness_perSample = richness/n_samples, #calculate richness per sample
         density_perSample = density_xbar/n_samples) #calculate density per sample
NOT RUN

Save output file: 

```{r}
write.csv(ecoSite_means, "cleaned_data/eco_richness_density.csv")
```

### 7. State-wide richness and density for each year:

#### A. Data Cleaning

##### i. Drop samples with <300 individuals; subsample those with >300 individuals to 300 individuals

Already completed above for ecoregion-level analyses - use "subsets_eco" as starting point

##### ii. Drop years with 10 or fewer sites: 

First, visualization of # sites per year in subsetted data (subsets_eco):

```{r}
subsets_eco%>%
  group_by(yr)%>%
  summarise(n_sites = length(unique(siteId)))%>%
  ggplot(aes(x = yr, y = n_sites))+
  geom_bar(stat = "identity", color = "black", fill = "grey")+
  geom_hline(aes(yintercept = 10), color = "red", linetype = "dashed")+
  theme_classic()+
  geom_text(aes(x = yr, y = n_sites, label = n_sites), size = 3, vjust = -0.2)+
  labs(x = "Year", y = "# of sites sampled")
```

Only 9 sites for most of the 90s - may need to adjust cutoffs. 

Apply 9-site minimum filter:

```{r}
statewide_9plus <- subsets_eco %>%
  group_by(yr)%>% #group by year
  mutate(n_sites = length(unique(siteId)))%>% #count n unique sites surveyed in that year
  ungroup()%>% #ungroup to retain all columns
  filter(!n_sites<9) #drop all years with fewer than 9 sites
```

#### B. Calculate state-wide density and richness by year:

Group by sample, calculate mean total sample density for each year:

```{r}
statewide_yearly <- statewide_9plus %>%
  group_by(sampleId)%>%
  summarise(richness = length(unique(scientificName)), 
            tot_dens = unique(tot_dens), 
            sampleDate = unique(sampleDate))
```

Save this dataframe for future analysis: 

```{r}
write.csv(statewide_yearly, "cleaned_data/statewide_richness_density.csv")
```

