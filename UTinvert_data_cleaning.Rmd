---
title: "UT Invert Data Cleaning"
author: "Gordon Gianniny"
date: "2023-05-18"
output: html_document
---

```{r include=FALSE}
#Loading packages:
library(tidyverse) #For data wrangling
library(lubridate) #For working with dates
library(ggplot2) #Plotting
library(RColorBrewer)
library(patchwork)
library(stringr)
library(vegan)
library(broom)
library(sp)
library(geonames) #for getting elevation data
library(rgbif) #for getting elevation data
```


## Overview

This script cleans and organizes aquatic invertebrate data obtained from the National Aquatic Monitoring Center (NAMC) for the state of Utah. The two input data files are: 

  * **Utah_Sample_Data_Final_comids.csv** - sample metadata for all samples. Key columns are: 
  
      - sampleId (unique to each sample), customerAbbreviation (unique to each sampling agency), siteID (unique to each site), siteLongitude and siteLatitude, sampleDate, sampleType, sampleMethod, sampleMethodId, habitatName, habitatID, area, fieldSplit, labSplit, mesh, COMID (unique to each stream segment)

  * **Utah_Taxa_Data_Final.csv** - taxa data for all samples. Key columns are: 
  
    - organismId (unique to each organism), sampleId, taxonomyId (unique to each taxa), levelId (unique to each classification level), levelName (finest level of classification), lifeStageId (unique to each lifestage), lifeStage, lifeStageAbbreviation, splitCount, labSplit, fieldSplit
    
Initial data cleaning objectives are to: 

1. Get rid of unneccesary columns in each dataset, then combine the two datasets (merge by sampleID)
  
2. Account for differences in lab split, sampling methods, etc. 
  
    A. Check lab split proportions across the dataset - what are the lab split values/bounds? Set a cutoff.  
  
    B. Sampling method, life stage, habitat type, and mesh size
  
    C. Classification levels - How much of the data is classified to genus? (Keeping all for now)
  
3. Calculate richness & deal with replicates:
  
    A. Calculate richness for each sample
  
    B. Average "true" replicates (within a day/ within 5 days)
  
    C. Average within-year (same siteId) samples
    
    D Average within-reach (same COMID) sites
  
4. Identify and extract sites with long-term (4+ year) data
  
    A. By site
  
    B. By COMID

Data cleaning steps from Rumschlag et al: 

  * Only use wadeable stream data
  
  * Control for different sampling methods: only use those that have been shown to produce similar results
  
  * Only use area-limited samples
  
  * Only include genera that were identified by both agencies (USGS and EPA)
  
  * Address changes in taxonomy: update old taxa names to the new taxonomy; create complexes of genera when unclear if updated or not. 
  * Address ambiguous naming and samples not ID'd to the genus level: Drop any obs. not ID'd to genus level; use genus level for samples with multiple species names (e.g. species1/species2), normalize any combined genus names - e.g., for "genus1/genus2", change all observations of "genus1" or "genus2" to "genus1/genus2". 
      
---
---

## I. Initial data cleaning objectives: 

### 1. Get rid of unneccesary columns in each dataset, then combine the two datasets (merge by sampleID)

**Metadata:** 

Read in the datasets, select key columns listed in intro: *(Note - also adjusting date formatting of sampleDate column to fix dates in the mid-90s coming in as dates in the future - e.g. instead of 1965-01-01, 2065-01-01)*

```{r}
metadata <- read.csv("raw_data/Utah_Sample_Data_Final_comids.csv")%>% #read in datafile
  select("sampleId" , "customerAbbreviation", "siteId", "siteLongitude", "siteLatitude", "sampleDate", "sampleType", "sampleMethod", "sampleMethodId", "habitatName", "habitatId", "area", "fieldSplit", "labSplit", "mesh", "COMID" )%>% #Selecting desired columns
  mutate(sampleDate = as.Date(sampleDate, format ="%m/%d/%y"),  #recode as date
         flag = ifelse(
           sampleDate > today(), 
           "N", "Y"), #flag all rows with dates greater than today
         sampleDate = as.character(sampleDate), #convert back to character to substring
         sampleDate= ifelse(flag == "N", str_replace(sampleDate, "20", "19"), sampleDate), #substring: correct century (replace "20" with "19") for all flagged rows 
         sampleDate = ymd(sampleDate)) #convert back to date format. 
           

taxa <- read.csv("raw_data/Utah_Taxa_Data_Final.csv")%>% #read in datafile
  select("organismId" , "sampleId", "taxonomyId", "scientificName", "levelId", "levelName", "phylum", "class", "order", "family", "subFamily", "genus", "species",  "lifeStageId", "lifeStage", "lifeStageAbbreviation", "splitCount", "labSplit", "fieldSplit") #selecting desired columns
```


Checking # samples in each dataframe: 

```{r}
length(unique(metadata$sampleId)) #number unique sample ID's in metadata
length(unique(taxa$sampleId)) #number unique sample ID's in taxa  data
```

160 more sample ID's in the metadata than in the taxa data, so should have metadata for all taxa samples. 

Merge the dataframes and re-code relevant colums as different classes: result should be 185,366 rows x 32 columns

```{r}
taxa_meta <- merge(taxa, metadata)%>%#merge
  mutate(
    customerAbbreviation = as.factor(customerAbbreviation), 
    levelName = as.factor(levelName), #recoding categorical columns as factors
    lifeStage = as.factor(lifeStage), #recoding categorical columns as factors
    #sampleDate = mdy(sampleDate), #recoding date column in r date format
    sampleType = as.factor(sampleType), #recoding categorical columns as factors
    sampleMethod = as.factor(sampleMethod), #recoding categorical columns as factors
    habitatName = as.factor(habitatName), #recoding categorical columns as factors
    habitatId = as.factor(habitatId)#recoding categorical columns as factors
  )
  
dim(taxa_meta) #check dimensions 

range(taxa_meta$sampleDate) #confirming that date range is believable
```

Save this dataframe: 

```{r}
write.csv(taxa_meta, "raw_data/ut_metadata_taxa_combined.csv")
```

### 2. Accounting for differences/checking accuracy in lab split, sampling methods, area, etc. 

#### A. Lab Split - what's the range? Do we need a cutoff? 

First pass, basic summary of lab split range: 

```{r}
summary(taxa_meta$labSplit)
```
  
Looks like there are a fair number of low lab splits, including some zeros - plotting: 

```{r}
labsplit.hist <- ggplot(metadata, aes(x = labSplit))+ #x axis = labSplit
  geom_histogram(color = "Black", fill = "Grey", bins = 70)+ 
  theme_classic()+
  geom_vline(xintercept = 0.055, linetype = "dashed", color = "Red")+ #add vertical line at 0.055 (cutoff used in Rumschlag et al.)
  labs(x = "Lab Split", y = "Count")
labsplit.hist
```

`r round((nrow(filter(metadata, labSplit < 0.055))/nrow(metadata))*100, 2)` percent of samples (n = `r nrow(filter(metadata, labSplit < 0.055))`) are below the 0.055 cutoff from Rumschlag et al (dashed red line). 


##### i. How many individuals are in the samples with lab split == 1? 

First, extract samples with labsplit == 1 and calculate total sample abundance:

```{r}
ls1 <- taxa_meta %>%
  filter(labSplit == 1)%>% #extract rows w/ labsplit = 1
  group_by(sampleId)%>% #group by sample ID
  summarise(sample_abund = sum(splitCount)) #calculate total abundance for each sample
```

Basic summary:

```{r}
summary(ls1$sample_abund)
```

Histogram: 

```{r}
ls1.hist <- ggplot(ls1, aes(x = sample_abund))+
  geom_histogram(color = "Black", fill = "Grey", bins = 70)+ 
  theme_classic()+
  annotate(geom = "label", x = 3000, y = 350, label = "Min = 1, Mean = 310, Max = 5,942")+
   annotate(geom = "label", x = 3000, y = 300, label = "32 samples w/ >1000 individuals; 11 samples w/ 1 individual")+
  labs(x = "Total Sample Abundance", y = "Count")
ls1.hist
```

Very strong left skew - most samples have fewer than ~1000 individuals. 

```{r}
length(which(ls1$sample_abund>1000))
length(which(ls1$sample_abund==1))
```

Only 32 samples with > 1000 individuals; 11 samples with only 1 individual. 

Save: 

```{r}
ggsave("plots/ls1_hist.pdf", ls1.hist, device = "pdf", units = "in", height = 5, width = 6.5, dpi = "retina")
```

##### ii. Applying lab split cutoff:

What are the tradeoffs between # samples and labsplit cutoff? 

Plot showing total # samples on Y, lab split on X. 

```{r}
n<- nrow(metadata)

ggplot(metadata, aes(x = labSplit))+
  stat_ecdf(aes(y = (1-after_stat(y))), geom = "line", size = 1, color = "red")+
  scale_y_continuous("Proportion of samples w/ Lab Split > x", position = "left", sec.axis = sec_axis(name = "# Samples w/ Lab Split > x", trans = ~.x*n))+
  theme_linedraw()+
  labs(x ="Lab Split")+
  geom_vline(xintercept = 0.1, linetype = "dashed")
  
```

For now - using the same cutoff as Rumschlag et al: 

```{r}
taxa_meta_labsplit <- taxa_meta %>%
  filter(!labSplit < 0.1) #remove all observations with labSplit < 0.055
```

Using this cutoff means we lose `r round(((nrow(taxa_meta)-nrow(taxa_meta_labsplit))/nrow(taxa_meta))*100, 2)` percent of observations (`r nrow(taxa_meta)-nrow(taxa_meta_labsplit)` individual observations). 

#### B. Sampling Method, Habitat type, life stage, and mesh size: 

First, how much variation is there in agency, sampling method, habitat type, etc? 

```{r}
categoricals <- taxa_meta_labsplit %>% 
  select_if(is.factor)%>% #extract all factor variables
  summarise_all(n_distinct)%>% #count # distinct levels in each factor
  mutate(n = row_number())%>% #extra col for pivot
  pivot_longer(!n, names_to = "variable", values_to = "n_levels")%>% #pivot longer to get column for variable names, column for # levels
  select(!n) #drop extra column
categoricals
```

16 different levels of identification, 6 life stages, 49 "customers" all samples are benthic, 3 sample methods and habitat types. Will probably want to check for variation between:

  * Different sampling methods
  
  * Different habitat types
  
  * Different agencies
  
  * Different mesh sizes (?)
  
Will also probably want to standardize which lifestages we use (ie include adults, or not?) and which level of classification we use (e.g. Rumschlag et al threw out anything not classified at least to genus)

**Rules as of 05-23**:

  * Keep all sampling methods, but keep in dataset throughout calculations. 
  
  * Keep all habitat types
  
  * Ignore agency for now
  
  * Keep all mesh sizes
  
  * Drop egg & exuvia; count adults for non-insects and insects in orders coleoptera & hemiptera. 
  
  * Assume that "unspecified" samples are larvae if insects, adults if non-insects - keep in dataset. 
  
  * Keep all levels of classification. 

**Identify different categories in each of these variables, how much data is in each, and clean up data according to the rules above:**


##### i. Life Stage: Drop exuvia, eggs, and unspecified

```{r}
ggplot(taxa_meta_labsplit, aes(x = lifeStage, fill = lifeStage))+
  geom_bar(stat = "count", color = "Black")+
  theme_classic()+
  labs(x = "Life Stage", y = "No. of Observations")+
  theme(legend.position = "none")
```

Almost all larvae and, some adults. Very few eggs and exuvia. 

Drop eggs & exuvia; assuming that unspecified samples are adults if non-insects, larvae if insects. Drop all insect adults unless in orders Coleoptera or Hemiptera.

First, make a list of the insect orders for which we want to drop observations of adults:

```{r}
insect_orders <- taxa_meta_labsplit%>%
  select(class, order)%>% #select class and order columns
  filter(class == "Insecta")%>% #filter all orders in class Insecta
  distinct()%>% #remove duplicates
  filter(!order == "Coleoptera", !order == "Hemiptera") #remove orders we want to keep (Coleoptera, Hemiptera)

to.drop <- insect_orders$order #create vector of order names to drop
```

Next, drop all eggs, exuvia, and adult insects in the to.drop list of orders:

```{r}
taxa_meta_stage <- taxa_meta_labsplit%>%
  filter(!lifeStage == "Egg", #drop all eggs
         !lifeStage == "Exuvia")%>% #drop all exuvia
  filter(!(lifeStage == "Adult"&order%in%to.drop)) #drop all adults from the orders in the to.drop list
```

##### ii. Sampling Method - Keep all for now:

```{r}
ggplot(taxa_meta_stage, aes(x = sampleMethod, fill = sampleMethod))+
  geom_bar(stat = "count", color = "Black")+
  theme_classic()+
  labs(x = "Sample Method", y = "No. of Observations")+
  theme(legend.position = "none")
```

Fairly evenly distibuted, lots of samples with each method. 

##### iii. Habitat Type - Keep all for now:

```{r}
ggplot(taxa_meta_stage, aes(x = habitatName, fill = habitatName))+
  geom_bar(stat = "count", color = "Black")+
  theme_classic()+
  labs(x = "Habitat Type", y = "No. of Observations")+
  theme(legend.position = "none")
```

Almost all targeted riffle, few "Riffle"

##### iv. Mesh Size - Keep all for now:

```{r}
ggplot(taxa_meta_stage, aes(x = factor(mesh), fill = factor(mesh)))+
  geom_bar(stat = "count", color = "Black")+
  theme_classic()+
  labs(x = "Mesh Size", y = "No. of Observations")+
  theme(legend.position = "none")
```

Quite a bit of variation, also lots of NAs. 

#### C. Classification Level - Keep all for now: 

**How much data is classified to genus?**

First - check which codes correspond with which lifestage

```{r}
level_codes <- taxa_meta_labsplit %>%
  select(levelName, levelId)%>% #extract level names and codes
  distinct()%>% #combine duplicate rows
  arrange(levelId) #sort by levelID
level_codes
```

Visual: 

```{r}
level.hist <- ggplot(taxa_meta_labsplit, aes(x = levelId))+ #x axis = levelId
  geom_histogram(fill = "Grey", color = "Black", bins = 15)+ #changing histogram color and bin size
  geom_vline(xintercept = 23, linetype = "dashed", color = "Red")+ #adding vertical line at 23 (code for genus)
  theme_classic()+
  labs( x = "Taxonomic Level ID", y = "Count") #axis labels
level.hist
```

Dashed red line = Genus - all samples with smaller taxonomic ID's are classified to higher levels. 

If we just used observations classified to genus or lower, we would lose `r round((nrow(filter(taxa_meta_labsplit, levelId < 23)) / nrow(taxa_meta_labsplit))*100, 2)` percent of the data, leaving  `r nrow(filter(taxa_meta_labsplit, levelId >= 23))` observations remaining. 

#### D. Checking area ranges; removing samples w/out area:

First pass - basic summary of sample area: 

```{r}
summary(metadata$area)
```

26 samples missing area, some samples with area of 100? 

Visual check: 

```{r}
ggplot(metadata, aes(x = area))+
  geom_histogram(color = "Black", fill = "Grey", bins = 70)+ 
  theme_classic()+
  labs(x = "Sample Area, m2", y = "Count")
```

Six samples have area = 100 - visualization without those six samples:

```{r}
ggplot(filter(metadata, area <50), aes(x = area))+
  geom_histogram(color = "Black", fill = "Grey", bins = 70)+ 
  theme_classic()+
  labs(x = "Sample Area, m2", y = "Count")
```

Much more believable. Removing samples with no area and with area == 100 from the dataset: 

```{r}
taxa_meta_area <- taxa_meta_stage%>%
  filter(!is.na(area), 
         !area == 100)
```

How many samples with area > "x":

```{r}
n<- nrow(metadata)

ggplot(filter(metadata, area<50), aes(x = area))+
  stat_ecdf(aes(y = (1-after_stat(y))), geom = "line", size = 1, color = "red")+
  scale_y_continuous("Proportion of samples w/ Area > x", position = "left", sec.axis = sec_axis(name = "# Samples w/ Area > x", trans = ~.x*n))+
  theme_linedraw()+
  labs(x ="Sample Area")+
  geom_vline(xintercept = 1, linetype = "dashed")+
  geom_vline(xintercept = 0.08, linetype = "dashed")
```


#### E. Calculate Density: 

Calculate density for each observation - density = (# organisms/(labSplit*fieldSplit))/area sampled

```{r}
taxa_meta_density <- taxa_meta_area %>%
  mutate(taxa_density = (splitCount/(labSplit*fieldSplit))/area)
```

Add site elevations: 

```{r}
tmd_sites <- taxa_meta_density%>%
  select(siteId, siteLongitude, siteLatitude)%>%
  distinct()

elev1 <- elevation(latitude = tmd_sites$siteLatitude, longitude = tmd_sites$siteLongitude, elevation_model = "srtm3", username = "ggianniny")%>% #calculate elevation using the SRTM3 model (uses GeoNames API)
  mutate(siteId = tmd_sites$siteId)%>% #add siteIDs 
  rename(elev_m = elevation_geonames)%>% #rename cols for merge
  select(siteId, elev_m)%>% #drop extra cols for merge
  distinct() #remove duplicates

taxa_meta_density2 <- merge(taxa_meta_density, elev1, all.x = T) #merge with taxonomic data
```

Save this file for future analysis: 

```{r}
write.csv(taxa_meta_density2, "cleaned_data/taxa_densities.csv")
```

### 3. Calculate Richness & Deal with Replicates

Assumptions for richness calculations are: 

  * We are only including larvae, pupae, and adults. 
  
  * We can treat all three sampling methods and habitat types as equivalent
  
  * All of the unique values of "scientificName" represent unique taxa (probably not true, but sticking with that for now)
  
  * We don't need to make any adjustments for area sampled or lab/field split ratios. 
  
  * We don't need to make any adjustments for variation in mesh size. 

#### A. Calculate Richness for each sample:

First, calculate species richness and total abundance for each sample: 

```{r}
sample_richness <- taxa_meta_density %>%
  group_by(sampleId, labSplit, fieldSplit, siteId, siteLongitude, siteLatitude, sampleMethod,habitatName, area, mesh, COMID, sampleDate)%>% #group by sampleId, plus keep other cols for future use. 
  summarise(richness = length(unique(scientificName)), #calculate richness = # unique values of scientificName in each sampleID
            tot_abundance = sum(splitCount), #calculate total abundance = sum of counts for all taxa in that sample
            tot_dens = sum(taxa_density)) #calculate total density = sum of density (individuals/m2) for all taxa in that sample
```
    
Save this file for future analyses: 

```{r}
write.csv(sample_richness, "cleaned_data/sample_richness.csv")
```

#### B. Average true replicates (samples from the same day or within one day of eachother).

Average all samples collected from the same site on the same day: 

```{r}
site_richness_day <- sample_richness %>%
  group_by(siteId, sampleDate, siteLongitude, siteLatitude, habitatName, area, mesh, COMID)%>% # group by siteId and sampleDate
  summarise(richness_xbar = mean(richness), #calculate mean richness
            abund_xbar = mean(tot_abundance), #calculate mean abundance
            density_xbar = mean(tot_dens), #calculate mean density
            labSplit = mean(labSplit), #mean labSplit across replicates
            fieldSplit = mean(fieldSplit) #mean fieldSplit across replicates
            #,sampleMethod = unique(sampleMethod) NOT RUN - keeping sampleMethod (results in keeping some samples seperate)
            ) 
```

Average all samples from the same site within **5 days** of each other: *(NOTE: haven't been able to figure out how to merge obs with in 1 day of eachother)*

```{r}
site_richness_5d <- site_richness_day %>%
  arrange(siteId, sampleDate)%>% #sort by siteId and sampleDate
  mutate(doy = yday(sampleDate), #new column with day of year of sample
         doy_round = (5*round(doy/5)), #round day of year to nearest 5
         yr = year(sampleDate), #new column with year of sample
         site_doy_yr = paste(siteId, doy_round, yr, sep = "_"))%>% #new column with combined string of siteId, rounded day of year, and sample year for grouping
  group_by(site_doy_yr, siteId, siteLongitude, siteLatitude, habitatName, area, mesh, COMID)%>% #group by siteId/doy/yr column, retain other cols
  summarise(richness_xbar = mean(richness_xbar), #calculate mean richness
            abund_xbar = mean(abund_xbar), #calculate mean abundance
             density_xbar = mean(density_xbar), #calculate mean density
            labSplit = mean(labSplit), #calculate mean labSplit
            fieldSplit = mean(fieldSplit), #calculate mean fieldSplit
            sampleDate = mean(sampleDate)) #calculate mean sampleDate

```

Check which rows were combined:

```{r}
sr5_comp <- site_richness_5d[,2:14] #get rid of extra column in new df for comparison
combined <- dplyr::setdiff(site_richness_day, sr5_comp) #new object with the 10 rows that are different between the daily dataframe and the 5 day dataframe
combined
```

All rows in the daily dataset that aren't in the 5-day dataset have sampleDates within 5 days of eachother, so appears to be working correctly. 

Save this file: 

```{r}
write.csv(sr5_comp, "cleaned_data/site_5day_richness.csv")
```

#### C. Average within-year samples

Calclulate averages:

```{r}
site_yr_richness <- sr5_comp %>%
  mutate(yr = year(sampleDate))%>% #add column for year
  group_by(siteId, yr)%>% #group by site and year
  summarise(richness_xbar = mean(richness_xbar), #calculate mean richness, abundance, lab/fieldsplit, area, and mesh. 
            abund_xbar = mean(abund_xbar), 
            density_xbar = mean(density_xbar), #calculate mean density
            labSplit = mean(labSplit), 
            fieldSplit = mean(fieldSplit), 
            area = mean(area), 
            mesh = mean(mesh, na.rm = T))
```

Add metadata back in:

```{r}
meta_sub <- metadata%>%
  select(siteId, siteLatitude, siteLongitude, COMID)%>% #select desired metadata cols
  distinct()

site_yr_richness2 <- merge(site_yr_richness, meta_sub) #merge with site x year richness data
```

Save file: 

```{r}
write.csv(site_yr_richness2, "cleaned_data/site_year_richness.csv") 
```

#### D. Average within-reach (same COMID) samples

Calculate average richness and abundance within each reach:

```{r}
comid_yr_richness <- site_yr_richness2 %>%
  group_by(COMID, yr)%>% #group by COMID, year
  summarise(richness_xbar = mean(richness_xbar), #calculate mean richnes, abundance, labSplit, fieldSplit, area, and mesh w/in each COMID * year combo. 
            abund_xbar = mean(abund_xbar),             
            density_xbar = mean(density_xbar), #calculate mean density
            labSplit = mean(labSplit), 
            fieldSplit = mean(fieldSplit), 
            area = mean(area), 
            mesh = mean(mesh, na.rm = T))
```

Save: 

```{r}
write.csv(comid_yr_richness, "cleaned_data/comid_year_richness.csv")
```

### 4. ID and seperate sites with longer records - minimum... 4 years of data?

#### A. By site: 

Calculate number of years of data at each site: 

```{r}
site_pors <- site_yr_richness2 %>%
  group_by(siteId)%>% #group by siteId
  summarise(por = length(unique(yr)))%>% #count # years data at that site
  arrange(desc(por)) #descending sort by period of record
```

Visualization: how many sites w/ longer-term datasets?  

```{r}
ggplot(site_pors, aes(x = factor(por), fill = factor(por)))+
  geom_bar(stat = "count", color = "Black")+
  theme_classic()+
  labs(x = "No. years of data", y = "No. sites")+
  theme(legend.position = "none")
```

Lots of sites have only 1 or 2 years of data - if use minimum 4 years, this would leave `r nrow(filter(site_pors, por >=4))` sites. 

**Extract subset of data w/ at least 4 years of data**: 

Extract site Id's for all sites with 4+ years of data:

```{r}
sites.4plus <- site_pors %>%
  filter(por >= 4)%>% #remove all sites with <4yrs data
  select(siteId) #drop extra column
```

Filter site * year richness data to only include these sites; add elevation data:  

```{r}
site_yr_longterm <- site_yr_richness2%>%
  filter(siteId%in%sites.4plus$siteId)#filter to drop all siteId's not in the sites.4plus list

elev <- elevation(latitude = site_yr_longterm$siteLatitude, longitude = site_yr_longterm$siteLongitude, elevation_model = "srtm3", username = "ggianniny")%>% #calculate elevation using the SRTM3 model (uses GeoNames API)
  mutate(siteId = site_yr_longterm$siteId)%>% #add siteIDs 
  rename(elev_m = elevation_geonames)%>% #rename cols for merge
  select(siteId, elev_m)%>% #drop extra cols for merge
  distinct() #remove duplicates

site_yr_longterm2 <- merge(site_yr_longterm, elev) #merge with taxonomic data
```

Save: 

```{r}
write.csv(site_yr_longterm2, "cleaned_data/longterm_site_richness.csv")
```

#### B. By COMID: 

Calculate number of years of data for each COMID: 

```{r}
comid_pors <- comid_yr_richness %>%
  group_by(COMID)%>% #group by COMID
  summarise(por = length(unique(yr)))%>% #count # years data at that COMID
  arrange(desc(por)) #descending sort by period of record
```

Visualization: how many sites w/ longer-term datasets?  

```{r}
ggplot(comid_pors, aes(x = factor(por), fill = factor(por)))+
  geom_bar(stat = "count", color = "Black")+
  theme_classic()+
  labs(x = "No. years of data", y = "No. COMIDs")+
  theme(legend.position = "none")
```

Lots of sites have only 1 or 2 years of data - if use minimum 4 years, this would leave `r nrow(filter(comid_pors, por >=4))` sites - so this adds an extra `r (nrow(filter(comid_pors, por >=4)))-(nrow(sites.4plus))` COMID's as compared to using the site-level data. 

**Extract subset of data w/ at least 4 years of data**: 

Extract site Id's for all sites with 4+ years of data:

```{r}
comid.4plus <- comid_pors %>%
  filter(por >= 4)%>% #remove all sites with <4yrs data
  select(COMID) #drop extra column
```

Filter site * year richness data to only include these sites: 

```{r}
comid_yr_longterm <- comid_yr_richness%>%
  filter(COMID%in%comid.4plus$COMID) #filter to drop all COMID's not in the comid.4plus list
```

Save: 

```{r}
write.csv(site_yr_longterm, "cleaned_data/longterm_comid_richness.csv")
```

### 5. Identify species of concern; create new dataframe with all SGCNs from UT and surrounding states:

#### A. UT SSGCNs:

Make a list of all invertebrate UT SGCNs (from https://wildlife.utah.gov/pdf/WAP/2022-05-sgcn-list.pdf):

```{r}
ut.sgcn.list <- c( "Pacifastacus gambelii", "Stygobromus utahensis","Oreohelix yavapai cummingsi", "Fossaria techella","Pyrgulopsis pilsbryana", "Pyrgulopsis peculiaris", "Pyrgulopsis plicata", "Oreohelix parawanensis", "Pyrgulopsis inopinata", "Physa megalochlamys", "Planorbella binneyi", "Gastrocopta quadridens", "Oreohelix peripherica", "Pyrgulopsis deserta", "Tryonia porrecta", "Oreohelix eurekensis", "Stagnicola bonnevillensis", "Fluminicola coloradoensis", "Pyrgulopsis hamlinensis", "Oxyloma kanabense", "Planorbella oregonensis ", "Pyrgulopsis lindahlae", "Pyrgulopsis anguina"," Oreohelix haydeni", "Oreohelix howardi","Vertigo concinnula", "Gastrocopta pilsbryana", "Stagnicola montanensis", "Pyrgulopsis variegata", "Pyrgulopsis nuwuvi", "Pyrgulopsis fusca", "Pyrgulopsis pinetorum", "Pupoides hordaceus", "Colligyrus greggi", "Succinea rusticana", "Pyrgulopsis santaclarensis", "Catinella stretchiana", "Gastrocopta ashmuni", "Pyrgulopsis chamberlini", "Ogaridiscus subrupicola", "Hawaiia neomexicana", "Pyrgulopsis saxatilis", "Vallonia perspectiva", "Pupilla syngenes","Physella utahensis", "Margaritifera falcata", "Physella zionis", "Stagnicola traski", "Anodonta nutalliana") 

ut_sgcn <- taxa_meta_density2 %>%
 filter(scientificName%in%ut.sgcn.list)%>%
  mutate(state = rep("UT", nrow(ut_sgcn)))

sort(unique(ut_sgcn$scientificName)) #return list of species present in this dataset. 
```

Four UT SGCNs present in the dataset. 

#### B. Idaho SGCNs:

Listed at https://www.landcan.org/pdfs/appendixB.pdf  

Make a vector with all ID SGCN scientific names; extract data with those taxa present: 

```{r}
id.sgcn.list <- c("Caurinella idahoensis","Ametropus ammophilus", "Centroptilum selanderorum", "Capnia zukeli", "Soyedina potteri", "Pictetiella expansa", "Agapetus montanus", "Anodonta californiensis", "Gonidea angulata", "Margaritifera falcata", "Oreohelix waltoni", "Oreohelix vortex", "Oreohelix strigosa goniogyra", "Oreohelix jugalis", "Oreohelix intersum", "Oreohelix idahoensis", "Oreohelix haydeni", "Polygyrella polygyrella", "Cryptomastix populi", "Cryptomastix harfordiana", "Cryptomastix mullani tuckeri", "Cryptomastix mullani latilabris", "Cryptomastix mullani clappi", "Cryptomastix mullani blandi", "Cryptomastix magnidentata", "Allogona ptychophora solida", "Allogona lombardii", "Pristiloma idahoense", "Udosarx lyrata", "Zacoleus idahoensis", "Prophysaon humile", "Hemphillia camelus", "Kootenaia burkei", " Discus marmorensis", "Anguispira nimapuna", "Helicodiscus salmonaceus", "Radiodiscus abietum", "Planogyra clappi", "Physa natricina", "Lanx", "Fisherola nuttalli", "Stagnicola montanensis", "Stagnicola idahoensis", "Stagnicola hinkleyi", "Valvata utahensis", "Taylorconcha serpenticola", "Pristinicola hemphilli", "Pyrgulopsis robusta", "Pyrgulopsis pilsbryana", "Pyrgulopsis bruneauensis", "Fluminicola coloradoensis")

id_sgcn <- taxa_meta_density2%>%
  filter(scientificName%in%id.sgcn.list)%>%
  mutate(state = rep("ID", nrow(id_sgcn)))

sort(unique(id_sgcn$scientificName))
```

5 Idaho SGCNs present in the dataset. 


#### C. Wyoming SGCNs:

List of all invertebrate SGCNs downloaded from:

https://wyndd.org/species_list/?wgfd_sgcn=Statewide&wy_occur=Regular%2CIrregular&wy_origin=Native%2CNonnative&res_status=Current&tax_level=Species%2CSubspecies%2FVariety&columns=sciname%2Ccomname%2Csynonyms%2Ctaxgroup%2Ctaxgroup2%2Cblm_wy%2Cg_rank%2Cs_rank%2Cusfws_esa%2Cusfs_sens%2Cusfs_solc%2Cwgfd_sgcn%2Cwy_contrib%2Cwyndd_soc%2Cwy_occur%2Cwy_origin&loc_settings=documentedObs%2Cdist_mod%2Cregular%2Cirregular&servicePrefix=prod_ 

Read in list of all inverts:

```{r}
wy.sgcn.list <- read.csv("Wyoming_SGCNS.csv", skip = 1)
```

Make new dataframe with all present in UT: 

```{r}
wy_sgcn <- taxa_meta_density2 %>%
  filter(scientificName%in%wy.sgcn.list$Scientific.Name)%>%
  mutate(state = rep("WY", nrow(wy_sgcn)))

sort(unique(wy_sgcn$scientificName))
```

11 Wyoming SGCNs present in the dataset

#### D. Colorado SGCNs: 

Make list of Colorado invertebrate SGCNs; filter out of full dataset: 

Mollusks from https://cpw.state.co.us/Documents/WildlifeSpecies/SWAP/CO_SWAP_Chapter2.pdf 
Invertebrates from https://cpw.state.co.us/Documents/WildlifeSpecies/SWAP/CO_SWAP_FULLVERSION.pdf (Appendix B, table B-1 starting on page B-1)

```{r}
co.sgcn.list <- c("Ferrissia walker",
"Promenetus umbillicatellus", "Anodontoides ferussacianus", "Ferrissia fragilis",
"Physa cupreonitens", "Uniomerus tetralasmus", "Acroloxus coloradensis", "Promenetus exacuous", "Physa gyrina utahensis", "Gumaga griseola", "Ochrotrichia susanae", "Ochrotrichia trapoiza", "Argia alberta", "Calopteryx aequabilis", "Calopteryx maculata", "Cordulegaster dorsalis", "Dromogomphus spoliatus", "Enallagma vesperum", "Epitheca petechialis", "Erpetogomphus compositus", "Erythemis vesiculosa", "Lestes alacer","Leucorrhinia glacialis", "Libellula nodisticta", "Somatochlora ensigera", "Somatochlora hudsonica", "Stylurus intricatus", "Sympetrum madidum","Acentrella parvula","Acerpenna pygmaea", "Ametropus neavei","Apobaetis etowah","Baetis brunneicolor", "Camelobaetidius warreni", "Ephemerella apopsis", "Homoeneuria alleni", "Labiobaetis apache", "Lachlania saskatchewanensis", "Macdunnoa persimplex", "Neochoroterpes oklahoma", "Plauditus cestus","Pseudiron centralis", "Rhithrogena flavianula","Arsapnia arapahoe", "Capnia arapahoe", "Capnia nelsoni","Mesocapnia frisoni", "Pteronarcys californica", "Nemomydas solitarius")

co_sgcn <- taxa_meta_density2%>%
  filter(scientificName%in%co.sgcn.list)%>%
  mutate(state = rep("CO", nrow(co_sgcn)))

sort(unique(co_sgcn$scientificName))
```

9 Colorado SGCNs present in the dataset. 

#### E. Arizona SGCNs

Full list downloaded from: https://awcs.azgfd.com/appendices/appendix-d-species-of-greatest-conservation-need-with-vulnerability-scores 

Read in datafile with all AZ SGCNs, extract invertebrates: 

```{r}
az.sgcn.list <- read.csv("AZ_SGCNs.csv")%>%
  filter(Taxonomic.Group == "Invertebrate")
```

Filter out observations of these taxa from the full dataset: 

```{r}
az_sgcn <- taxa_meta_density2%>%
  filter(scientificName%in%az.sgcn.list$Scientific.Name)%>%
  mutate(state = rep("AZ", nrow(az_sgcn)))

sort(unique(az_sgcn$scientificName))
```

5 AZ SGCNs in the dataset. 

#### F. Nevada SGCNs: 

Make a list of all Nevada invertebrate SGCNs (from https://www.ndow.org/wp-content/uploads/2022/01/2013-NV-WAP-Complete-NOT-ADA.pdf, table starting on page 73)

```{r}
nv.sgcn.list <- c("Anodonta californiensis","Tryonia variegata", "Pyrgulopsis pellita", "Pyrgulopsis erythropoma", "Pyrgulopsis peculiaris", "Pyrgulopsis papillata", "Pyrgulopsis coloradensis", "Pyrgulopsis lata", "Pyrgulopsis montana", "Pyrgulopsis fausta", "Pyrgulopsis crystalis", "Pyrgulopsis nanus", "Pyrgulopsis dixensis", "Pyrgulopsis aloba", "Pyrgulopsis villacampae", "Pyrgulopsis leporina", "Pyrgulopsis augustae", "Pyrgulopsis notidicola", "Pyrgulopsis isolata", "Pyrgulopsis gracilis", "Pyrgulopsis fairbanksensis", "Pyrgulopsis breviloba", "Pyrgulopsis planulata", "Pyrgulopsis bruesi", "Tryonia clathrata", "Pyrgulopsis marcida", "Pyrgulopsis hubbsi", "Pyrgulopsis humboldtensis", "Pyrgulopsis sathos", "Pyrgulopsis wongi","Pyrgulopsis imperialis", "Pyrgulopsis sublata", "Pyrgulopsis landyei", "Pyrgulopsis basiglans", "Pyrgulopsis lockensis", "Pyrgulopsis anguina", "Pyrgulopsis pisteri", "Tryonia ericae", "Pyrgulopsis avernalis", "Pyrgulopsis carinifera", "Tryonia monitorae", "Pyrgulopsis neritella", "Pyrgulopsis militaris", "Pyrgulopsis serrata", "Pyrgulopsis variegata", "Pyrgulopsis micrococcus", "Pyrgulopsis pictilis", "Pyrgulopsis merriami", "Pyrgulopsis aurata", "Tryonia elata","Fluminicola dalli", "Pyrgulopsis sadai", "Pyrgulopsis bifurcata", "Juga interioris", "Pyrgulopsis turbatrix", "Pyrgulopsis anatina", "Pyrgulopsis umbilicata", "Pyrgulopsis sulcata", "Tryonia angulata", "Pyrgulopsis deaconi", "Pyrgulopsis limaria", "Eremopyrgus eganensis", "Pyrgulopsis sterilis", "Pyrgulopsis orbiculata", "Pyrgulopsis cruciglans", "Fluminicola turbiniformis", "Pyrgulopsis millenaria", "Pyrgulopsis hovinghi", "Pyrgulopsis vinyardi", "Fluminicola virginius")

nv_sgcn <- taxa_meta_density2 %>%
  filter(scientificName%in%nv.sgcn.list)%>%
  mutate(state = "NV")

sort(unique(nv_sgcn$scientificName))
```

Only 1 NV SGCN present in dataset. 

#### G. Combine all states and save: 

```{r}
all_sgcn <- rbind(ut_sgcn, id_sgcn, wy_sgcn, co_sgcn, az_sgcn, nv_sgcn) #rowbind data from all 6 states


#Combine states for taxa that are listed in multiple states (e.g. instead of a row for Taxa 1/ UT and another row for Taxa 1/ NV, want 1 row with Taxa 1 / UT,NV)

sgcn_states <- all_sgcn%>% 
  distinct(scientificName, state)%>% #extract states and taxa names
  arrange(scientificName)%>% #sort by taxa name
  group_by(scientificName)%>% # group by taxa name
  summarise(states = toString(unique(state))) #create new "states" column with a character string of all unique values of "state" within that taxa

all_sgcn_states <- merge(all_sgcn, sgcn_states)%>% #merge with the full SGCN dataset
  select(!state)%>% #drop the old "state" column
  distinct() #drop duplicate rows
  
sort(unique(all_sgcn_states$scientificName)) #return list of all SGCN taxa present in the data
```

Total of 24 SGCNs that are listed in different states. **NOTE - many of these taxa are present in the data but have splitCounts/densities of 0... When dropping taxa with splitCount == 0, only have 10 SGCNs in the dataset**

Save this file for future analysis: 

```{r}
write.csv(all_sgcn_states,"cleaned_data/sgcns.csv")
```

---
---

## II. Data cleaning steps from Rumschlag et al: 

### 1. Only use wadeable stream data
  
### 2. Control for different sampling methods: only use those that have been shown to produce similar results

### 3. Only use area-limited samples
  
### 4. Only include genera that were identified by both agencies (USGS and EPA)
  
### 5. Address changes in taxonomy: 

#### A. Update old taxa names to the new taxonomy

#### B. Create complexes of genera when unclear if updated or not. 

### 6. Address ambiguous naming and samples not ID'd to the genus level: (except midges, etc.)

#### A. Drop any obs. not ID'd to genus level

#### B. Use genus level for samples with multiple species names (e.g. species1/species2)

#### C. Normalize any combined genus names - e.g., for "genus1/genus2", change all observations of "genus1" or "genus2" to "genus1/genus2".

